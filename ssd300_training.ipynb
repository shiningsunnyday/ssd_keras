{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD300 Training Tutorial\n",
    "\n",
    "This tutorial explains how to train an SSD300 on the Pascal VOC datasets. The preset parameters reproduce the training of the original SSD300 \"07+12\" model. Training SSD512 works simiarly, so there's no extra tutorial for that. The same goes for training on other datasets.\n",
    "\n",
    "You can find a summary of a full training here to get an impression of what it should look like:\n",
    "[SSD300 \"07+12\" training summary](https://github.com/pierluigiferrari/ssd_keras/blob/master/training_summaries/ssd300_pascal_07%2B12_training_summary.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 9 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales = scales_pascal\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "K.clear_session() # Clear previous models from memory.\n",
    "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=mean_color,\n",
    "                swap_channels=swap_channels)\n",
    "weights_path = 'base_models/VOC_coco_SSD_Belga_Relabelled.h5'\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "# adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "# model.compile(optimizer=sgd, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 15:42:45.738596 139897547109568 deprecation_wrapper.py:119] From /usr/lib64/python2.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0802 15:42:45.758625 139897547109568 deprecation.py:323] From keras_loss_function/keras_ssd_loss.py:133: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0802 15:42:45.770226 139897547109568 deprecation.py:323] From keras_loss_function/keras_ssd_loss.py:74: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0802 15:42:45.785873 139897547109568 deprecation.py:323] From keras_loss_function/keras_ssd_loss.py:166: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "freeze_range = range(4,18) # layers to freeze, currently first four blocks\n",
    "for i in freeze_range:\n",
    "    model.layers[i].trainable = False\n",
    "model.compile(optimizer=sgd, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image set 'data2.txt': 100%|██████████| 6692/6692 [00:31<00:00, 214.46it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "images_dir = '../datasets/images'\n",
    "labels_file_train = '../datasets/belgas_relabelled.csv'\n",
    "labels_file_test = '../datasets/belgas_relabelled_test.csv'\n",
    "\n",
    "classes = ['Adidas',\n",
    " 'Base',\n",
    " 'Citroen-text',\n",
    " 'Dexia',\n",
    " 'Kia',\n",
    " 'Nike',\n",
    " 'Puma',\n",
    " 'Shell',\n",
    " 'TNT',\n",
    " 'Umbro'] # Just so we can print class names onto the image instead of IDs\n",
    "\n",
    "train_dataset.parse_csv(images_dir,labels_file_train,['image_name','xmin','xmax','ymin','ymax','class_id'])\n",
    "val_dataset.parse_csv(images_dir,labels_file_test,['image_name','xmin','xmax','ymin','ymax','class_id'])\n",
    "\n",
    "# train_dataset.create_hdf5_dataset(file_path='../datasets/belgas_relabelled_train_dataset.h5',\n",
    "#                                   resize=False,\n",
    "#                                   variable_image_size=True,\n",
    "#                                   verbose=True)\n",
    "#\n",
    "# val_dataset.create_hdf5_dataset(file_path='../datasets/belgas_relabelled_val_dataset.h5',\n",
    "#                                 resize=False,\n",
    "#                                 variable_image_size=True,\n",
    "#                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     7,
     18,
     25,
     43,
     50
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training dataset:\t  6692\n",
      "Number of images in the validation dataset:\t     0\n"
     ]
    }
   ],
   "source": [
    "# 3: Set the batch size.\n",
    "\n",
    "batch_size = 32 # Change the batch size if you like, or if you run into GPU memory issues.\n",
    "\n",
    "# 4: Set the image transformations for pre-processing and data augmentation options.\n",
    "\n",
    "# For the training generator:\n",
    "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
    "                                            img_width=img_width,\n",
    "                                            background=mean_color)\n",
    "\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [model.get_layer('conv4_3_norm_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('fc7_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv6_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv7_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv8_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv9_2_mbox_conf').output_shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[ssd_data_augmentation],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define a learning rate schedule.\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    return 0.0001\n",
    "    if epoch < 2:\n",
    "        return 0.001\n",
    "    elif epoch < 5:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     18
    ]
   },
   "outputs": [],
   "source": [
    "# Define model callbacks.\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath='ssd300_logos_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
    "                                   monitor='loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "\n",
    "csv_logger = CSVLogger(filename='ssd300_logos_training_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule,\n",
    "                                                verbose=1)\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             learning_rate_scheduler,\n",
    "             terminate_on_nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reproduce the training of the \"07+12\" model mentioned above, at 1,000 training steps per epoch you'd have to train for 120 epochs. That is going to take really long though, so you might not want to do all 120 epochs in one go and instead train only for a few epochs at a time. You can find a summary of a full training [here](https://github.com/pierluigiferrari/ssd_keras/blob/master/training_summaries/ssd300_pascal_07%2B12_training_summary.md).\n",
    "\n",
    "In order to only run a partial training and resume smoothly later on, there are a few things you should note:\n",
    "1. Always load the full model if you can, rather than building a new model and loading previously saved weights into it. Optimizers like SGD or Adam keep running averages of past gradient moments internally. If you always save and load full models when resuming a training, then the state of the optimizer is maintained and the training picks up exactly where it left off. If you build a new model and load weights into it, the optimizer is being initialized from scratch, which, especially in the case of Adam, leads to small but unnecessary setbacks every time you resume the training with previously saved weights.\n",
    "2. In order for the learning rate scheduler callback above to work properly, `fit_generator()` needs to know which epoch we're in, otherwise it will start with epoch 0 every time you resume the training. Set `initial_epoch` to be the next epoch of your training. Note that this parameter is zero-based, i.e. the first epoch is epoch 0. If you had trained for 10 epochs previously and now you'd want to resume the training from there, you'd set `initial_epoch = 10` (since epoch 10 is the eleventh epoch). Furthermore, set `final_epoch` to the last epoch you want to run. To stick with the previous example, if you had trained for 10 epochs previously and now you'd want to train for another 10 epochs, you'd set `initial_epoch = 10` and `final_epoch = 20`.\n",
    "3. In order for the model checkpoint callback above to work correctly after a kernel restart, set `model_checkpoint.best` to the best validation loss from the previous training. If you don't do this and a new `ModelCheckpoint` object is created after a kernel restart, that object obviously won't know what the last best validation loss was, so it will always save the weights of the first epoch of your new training and record that loss as its new best loss. This isn't super-important, I just wanted to mention it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "# If you're resuming a previous training, set `initial_epoch` and `final_epoch` accordingly.\n",
    "initial_epoch   = 0\n",
    "final_epoch     = 50\n",
    "steps_per_epoch = 100\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=train_generator,\n",
    "                              validation_steps=ceil(train_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make predictions\n",
    "\n",
    "Now let's make some predictions on the validation dataset with the trained model. For convenience we'll use the validation generator that we've already set up above. Feel free to change the batch size.\n",
    "\n",
    "You can set the `shuffle` option to `False` if you would like to check the model's progress on the same image(s) over the course of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "# # # 1: Set the generator for the predictions.\n",
    "\n",
    "# predict_generator = val_dataset.generate(batch_size=100,\n",
    "#                                          shuffle=True,\n",
    "#                                          transformations=[convert_to_3_channels,\n",
    "#                                                           resize],\n",
    "#                                          label_encoder=None,\n",
    "#                                          returns={'processed_images',\n",
    "#                                                   'filenames',\n",
    "#                                                   'inverse_transform',\n",
    "#                                                   'original_images',\n",
    "#                                                   'original_labels'},\n",
    "#                                          keep_images_without_gt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 2: Generate samples.\n",
    "\n",
    "# batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(predict_generator)\n",
    "\n",
    "# i = 0 # Which batch item to look at\n",
    "\n",
    "# print(\"Image:\", batch_filenames[i])\n",
    "# print()\n",
    "# print(\"Ground truth boxes:\\n\")\n",
    "# print(np.array(batch_original_labels[i]))\n",
    "\n",
    "# # 3: Make predictions.\n",
    "\n",
    "# y_pred = model.predict(batch_images)\n",
    "# print(y_pred)\n",
    "\n",
    "# # 4: Decode the raw predictions in `y_pred`.\n",
    "\n",
    "# y_pred_decoded = decode_detections(y_pred,\n",
    "#                                    confidence_thresh=0.2,\n",
    "#                                    iou_threshold=0.4,\n",
    "#                                    top_k=200,\n",
    "#                                    normalize_coords=normalize_coords,\n",
    "#                                    img_height=img_height,\n",
    "#                                    img_width=img_width)\n",
    "# print(y_pred_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made the predictions on the resized images, but we'd like to visualize the outcome on the original input images, so we'll convert the coordinates accordingly. Don't worry about that opaque `apply_inverse_transforms()` function below, in this simple case it just aplies `(* original_image_size / resized_image_size)` to the box coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5: Convert the predictions for the original image.\n",
    "\n",
    "# y_pred_decoded_inv = apply_inverse_transforms(y_pred_decoded, batch_inverse_transforms)\n",
    "\n",
    "# np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
    "# print(\"Predicted boxes:\\n\")\n",
    "# print('   class   conf xmin   ymin   xmax   ymax')\n",
    "# while y_pred_decoded_inv[i].size == 0:\n",
    "#     i+=1\n",
    "    \n",
    "# i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's draw the predicted boxes onto the image. Each predicted box says its confidence next to the category name. The ground truth boxes are also drawn onto the image in green for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "# # 5: Draw the predicted boxes onto the image\n",
    "\n",
    "# # Set the colors for the bounding boxes\n",
    "# colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\n",
    "# classes = ['Adidas',\n",
    "#  'Adidas-text',\n",
    "#  'Airness',\n",
    "#  'BFGoodrich',\n",
    "#  'Base',\n",
    "#  'Bik',\n",
    "#  'Bouigues',\n",
    "#  'Bridgestone',\n",
    "#  'Bridgestone-text',\n",
    "#  'Carglass',\n",
    "#  'Citroen',\n",
    "#  'Citroen-text',\n",
    "#  'CocaCola',\n",
    "#  'Cofidis',\n",
    "#  'Dexia',\n",
    "#  'ELeclerc',\n",
    "#  'Ferrari',\n",
    "#  'Gucci',\n",
    "#  'Kia',\n",
    "#  'Mercedes',\n",
    "#  'Nike',\n",
    "#  'Peugeot',\n",
    "#  'Puma',\n",
    "#  'Puma-text',\n",
    "#  'Quick',\n",
    "#  'Reebok',\n",
    "#  'Roche',\n",
    "#  'SNCF',\n",
    "#  'Shell',\n",
    "#  'Standard_Liege',\n",
    "#  'StellaArtois',\n",
    "#  'TNT',\n",
    "#  'Total',\n",
    "#  'US_President',\n",
    "#  'Umbro',\n",
    "#  'VRT',\n",
    "#  'Veolia'] # Just so we can print class names onto the image instead of IDs\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(20,12))\n",
    "# plt.imshow(batch_original_images[i])\n",
    "\n",
    "# current_axis = plt.gca()\n",
    "# for box in batch_original_labels[i]:\n",
    "#     xmin = box[1]\n",
    "#     ymin = box[2]\n",
    "#     xmax = box[3]\n",
    "#     ymax = box[4]\n",
    "#     label = '{}'.format(classes[int(box[0])])\n",
    "#     current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n",
    "#     current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n",
    "\n",
    "# for box in y_pred_decoded_inv[i]:\n",
    "#     xmin = box[2]\n",
    "#     ymin = box[3]\n",
    "#     xmax = box[4]\n",
    "#     ymax = box[5]\n",
    "#     color = colors[int(box[0])]\n",
    "#     label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
    "#     current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
    "#     current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssd_keras",
   "language": "python",
   "name": "ssd_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
